{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n",
    "    batch_size, channels_count, input_height, input_width = input_matrix_shape\n",
    "    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "    return batch_size, out_channels, output_height, output_width\n",
    "\n",
    "\n",
    "class ABCConv2d(ABC):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.kernel = kernel\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, input_tensor):\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n",
    "    out_channels = kernel.shape[0]\n",
    "    in_channels = kernel.shape[1]\n",
    "    kernel_size = kernel.shape[2]\n",
    "\n",
    "    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n",
    "    layer.set_kernel(kernel)\n",
    "\n",
    "    return layer(input_matrix)\n",
    "\n",
    "\n",
    "class Conv2d(ABCConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding=0, bias=False)\n",
    "\n",
    "    def set_kernel(self, kernel):\n",
    "        self.conv2d.weight.data = kernel\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        return self.conv2d(input_tensor)\n",
    "\n",
    "\n",
    "def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n",
    "                      input_height=4, input_width=4, stride=2):\n",
    "    kernel = torch.tensor(\n",
    "                      [[[[0., 1, 0],\n",
    "                         [1,  2, 1],\n",
    "                         [0,  1, 0]],\n",
    "\n",
    "                        [[1, 2, 1],\n",
    "                         [0, 3, 3],\n",
    "                         [0, 1, 10]],\n",
    "\n",
    "                        [[10, 11, 12],\n",
    "                         [13, 14, 15],\n",
    "                         [16, 17, 18]]]])\n",
    "\n",
    "    in_channels = kernel.shape[1]\n",
    "\n",
    "    input_tensor = torch.arange(0, batch_size * in_channels *\n",
    "                                input_height * input_width,\n",
    "                                out=torch.FloatTensor()) \\\n",
    "        .reshape(batch_size, in_channels, input_height, input_width)\n",
    "\n",
    "    custom_conv2d_out = create_and_call_conv2d_layer(\n",
    "        conv2d_layer_class, stride, kernel, input_tensor)\n",
    "    conv2d_out = create_and_call_conv2d_layer(\n",
    "        Conv2d, stride, kernel, input_tensor)\n",
    "\n",
    "    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n",
    "             and (custom_conv2d_out.shape == conv2d_out.shape)\n",
    "\n",
    "\n",
    "class Conv2dMatrixV2(ABCConv2d):\n",
    "    # Функция преобразования кернела в нужный формат.\n",
    "    def _convert_kernel(self):\n",
    "        out_channels, in_channels, kernel_size, _ = self.kernel.shape\n",
    "        converted_kernel = self.kernel.view((out_channels,-1)) # Реализуйте преобразование кернела.\n",
    "        return converted_kernel\n",
    "\n",
    "    # Функция преобразования входа в нужный формат.\n",
    "    def _convert_input(self, torch_input, output_height, output_width):\n",
    "        out_channels, in_channels, k_size, _ = self.kernel.shape\n",
    "        batch_size, in_channels, in_height, in_width = torch_input.shape\n",
    "        stride = self.stride\n",
    "\n",
    "        # Реализуйте преобразование входа.\n",
    "\n",
    "        # number of columns in a single reshaped image\n",
    "        for i in range(output_height*output_width) :\n",
    "            col = (i % output_height)*stride\n",
    "            row = (i // output_height)*stride \n",
    "            converted_input[:,i::output_height*output_width] = torch_input[:,:,row:row+k_size, col:col+k_size].reshape(batch_size,-1).permute(1,0)\n",
    "            \n",
    "\n",
    "        return converted_input\n",
    "\n",
    "    def __call__(self, torch_input):\n",
    "        batch_size, out_channels, output_height, output_width\\\n",
    "            = calc_out_shape(\n",
    "                input_matrix_shape=torch_input.shape,\n",
    "                out_channels=self.kernel.shape[0],\n",
    "                kernel_size=self.kernel.shape[2],\n",
    "                stride=self.stride,\n",
    "                padding=0)\n",
    "\n",
    "        converted_kernel = self._convert_kernel()\n",
    "        converted_input = self._convert_input(torch_input, output_height, output_width)\n",
    "\n",
    "        conv2d_out_alternative_matrix_v2 = converted_kernel @ converted_input\n",
    "        return conv2d_out_alternative_matrix_v2.transpose(1,0).view(torch_input.shape[0],\n",
    "                                                     self.out_channels, \n",
    "                                                     output_height,\n",
    "                                                     output_width)\n",
    "\n",
    "# Проверка происходит автоматически вызовом следующего кода\n",
    "# (раскомментируйте для самостоятельной проверки,\n",
    "#  в коде для сдачи задания должно быть закомментировано):\n",
    "print(test_conv2d_layer(Conv2dMatrixV2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test reshape kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2],\n",
      "          [ 3,  4,  5]],\n",
      "\n",
      "         [[ 6,  7,  8],\n",
      "          [ 9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14],\n",
      "          [15, 16, 17]]],\n",
      "\n",
      "\n",
      "        [[[18, 19, 20],\n",
      "          [21, 22, 23]],\n",
      "\n",
      "         [[24, 25, 26],\n",
      "          [27, 28, 29]],\n",
      "\n",
      "         [[30, 31, 32],\n",
      "          [33, 34, 35]]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\n"
     ]
    }
   ],
   "source": [
    "kern = torch.arange(36).reshape((2,3,2,3))\n",
    "print(kern)\n",
    "\n",
    "kern_reshape = kern.view((2,-1))\n",
    "print(kern_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0,   1,   2,   3,   4],\n",
      "          [  5,   6,   7,   8,   9],\n",
      "          [ 10,  11,  12,  13,  14],\n",
      "          [ 15,  16,  17,  18,  19],\n",
      "          [ 20,  21,  22,  23,  24]],\n",
      "\n",
      "         [[ 25,  26,  27,  28,  29],\n",
      "          [ 30,  31,  32,  33,  34],\n",
      "          [ 35,  36,  37,  38,  39],\n",
      "          [ 40,  41,  42,  43,  44],\n",
      "          [ 45,  46,  47,  48,  49]],\n",
      "\n",
      "         [[ 50,  51,  52,  53,  54],\n",
      "          [ 55,  56,  57,  58,  59],\n",
      "          [ 60,  61,  62,  63,  64],\n",
      "          [ 65,  66,  67,  68,  69],\n",
      "          [ 70,  71,  72,  73,  74]]],\n",
      "\n",
      "\n",
      "        [[[ 75,  76,  77,  78,  79],\n",
      "          [ 80,  81,  82,  83,  84],\n",
      "          [ 85,  86,  87,  88,  89],\n",
      "          [ 90,  91,  92,  93,  94],\n",
      "          [ 95,  96,  97,  98,  99]],\n",
      "\n",
      "         [[100, 101, 102, 103, 104],\n",
      "          [105, 106, 107, 108, 109],\n",
      "          [110, 111, 112, 113, 114],\n",
      "          [115, 116, 117, 118, 119],\n",
      "          [120, 121, 122, 123, 124]],\n",
      "\n",
      "         [[125, 126, 127, 128, 129],\n",
      "          [130, 131, 132, 133, 134],\n",
      "          [135, 136, 137, 138, 139],\n",
      "          [140, 141, 142, 143, 144],\n",
      "          [145, 146, 147, 148, 149]]],\n",
      "\n",
      "\n",
      "        [[[150, 151, 152, 153, 154],\n",
      "          [155, 156, 157, 158, 159],\n",
      "          [160, 161, 162, 163, 164],\n",
      "          [165, 166, 167, 168, 169],\n",
      "          [170, 171, 172, 173, 174]],\n",
      "\n",
      "         [[175, 176, 177, 178, 179],\n",
      "          [180, 181, 182, 183, 184],\n",
      "          [185, 186, 187, 188, 189],\n",
      "          [190, 191, 192, 193, 194],\n",
      "          [195, 196, 197, 198, 199]],\n",
      "\n",
      "         [[200, 201, 202, 203, 204],\n",
      "          [205, 206, 207, 208, 209],\n",
      "          [210, 211, 212, 213, 214],\n",
      "          [215, 216, 217, 218, 219],\n",
      "          [220, 221, 222, 223, 224]]],\n",
      "\n",
      "\n",
      "        [[[225, 226, 227, 228, 229],\n",
      "          [230, 231, 232, 233, 234],\n",
      "          [235, 236, 237, 238, 239],\n",
      "          [240, 241, 242, 243, 244],\n",
      "          [245, 246, 247, 248, 249]],\n",
      "\n",
      "         [[250, 251, 252, 253, 254],\n",
      "          [255, 256, 257, 258, 259],\n",
      "          [260, 261, 262, 263, 264],\n",
      "          [265, 266, 267, 268, 269],\n",
      "          [270, 271, 272, 273, 274]],\n",
      "\n",
      "         [[275, 276, 277, 278, 279],\n",
      "          [280, 281, 282, 283, 284],\n",
      "          [285, 286, 287, 288, 289],\n",
      "          [290, 291, 292, 293, 294],\n",
      "          [295, 296, 297, 298, 299]]],\n",
      "\n",
      "\n",
      "        [[[300, 301, 302, 303, 304],\n",
      "          [305, 306, 307, 308, 309],\n",
      "          [310, 311, 312, 313, 314],\n",
      "          [315, 316, 317, 318, 319],\n",
      "          [320, 321, 322, 323, 324]],\n",
      "\n",
      "         [[325, 326, 327, 328, 329],\n",
      "          [330, 331, 332, 333, 334],\n",
      "          [335, 336, 337, 338, 339],\n",
      "          [340, 341, 342, 343, 344],\n",
      "          [345, 346, 347, 348, 349]],\n",
      "\n",
      "         [[350, 351, 352, 353, 354],\n",
      "          [355, 356, 357, 358, 359],\n",
      "          [360, 361, 362, 363, 364],\n",
      "          [365, 366, 367, 368, 369],\n",
      "          [370, 371, 372, 373, 374]]],\n",
      "\n",
      "\n",
      "        [[[375, 376, 377, 378, 379],\n",
      "          [380, 381, 382, 383, 384],\n",
      "          [385, 386, 387, 388, 389],\n",
      "          [390, 391, 392, 393, 394],\n",
      "          [395, 396, 397, 398, 399]],\n",
      "\n",
      "         [[400, 401, 402, 403, 404],\n",
      "          [405, 406, 407, 408, 409],\n",
      "          [410, 411, 412, 413, 414],\n",
      "          [415, 416, 417, 418, 419],\n",
      "          [420, 421, 422, 423, 424]],\n",
      "\n",
      "         [[425, 426, 427, 428, 429],\n",
      "          [430, 431, 432, 433, 434],\n",
      "          [435, 436, 437, 438, 439],\n",
      "          [440, 441, 442, 443, 444],\n",
      "          [445, 446, 447, 448, 449]]]])\n",
      "torch.Size([6, 3, 3, 3])\n",
      "tensor([[  0,  75, 150, 225, 300, 375],\n",
      "        [  1,  76, 151, 226, 301, 376],\n",
      "        [  2,  77, 152, 227, 302, 377],\n",
      "        [  5,  80, 155, 230, 305, 380],\n",
      "        [  6,  81, 156, 231, 306, 381],\n",
      "        [  7,  82, 157, 232, 307, 382],\n",
      "        [ 10,  85, 160, 235, 310, 385],\n",
      "        [ 11,  86, 161, 236, 311, 386],\n",
      "        [ 12,  87, 162, 237, 312, 387],\n",
      "        [ 25, 100, 175, 250, 325, 400],\n",
      "        [ 26, 101, 176, 251, 326, 401],\n",
      "        [ 27, 102, 177, 252, 327, 402],\n",
      "        [ 30, 105, 180, 255, 330, 405],\n",
      "        [ 31, 106, 181, 256, 331, 406],\n",
      "        [ 32, 107, 182, 257, 332, 407],\n",
      "        [ 35, 110, 185, 260, 335, 410],\n",
      "        [ 36, 111, 186, 261, 336, 411],\n",
      "        [ 37, 112, 187, 262, 337, 412],\n",
      "        [ 50, 125, 200, 275, 350, 425],\n",
      "        [ 51, 126, 201, 276, 351, 426],\n",
      "        [ 52, 127, 202, 277, 352, 427],\n",
      "        [ 55, 130, 205, 280, 355, 430],\n",
      "        [ 56, 131, 206, 281, 356, 431],\n",
      "        [ 57, 132, 207, 282, 357, 432],\n",
      "        [ 60, 135, 210, 285, 360, 435],\n",
      "        [ 61, 136, 211, 286, 361, 436],\n",
      "        [ 62, 137, 212, 287, 362, 437]])\n",
      "torch.Size([27, 6])\n",
      "torch.Size([27, 6])\n",
      "tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  75.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0., 150.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0., 225.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "        300.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 375.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.])\n"
     ]
    }
   ],
   "source": [
    "input_img = torch.arange(6*3*5*5).reshape((6,3,5,5))\n",
    "print(input_img)\n",
    "\n",
    "print(input_img[:,:,0:3, 0:3].shape)\n",
    "print( input_img[:,:,0:3, 0:3].reshape(6,-1).permute(1,0) )\n",
    "print( input_img[:,:,0:3, 0:3].reshape(6,-1).permute(1,0).shape )\n",
    "# let stride=1\n",
    "# let kernel.size = (2,3,3,3)\n",
    "input_reshaped = torch.zeros((3*3*3,6*3*3))\n",
    "print(input_reshaped[:,::3*3].shape)\n",
    "\n",
    "input_reshaped[:,::3*3] = input_img[:,:,0:3, 0:3].reshape(6,-1).permute(1,0)\n",
    "print(input_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be1c9af88beea7cbf0c54e199f2658eb520b0d9efe683bd43be70979af5f4066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
